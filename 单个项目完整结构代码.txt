é¡¹ç›® 'rawbot-2api' çš„ç»“æ„æ ‘:
ğŸ“‚ rawbot-2api/
    ğŸ“„ .env
    ğŸ“„ .env.example
    ğŸ“„ Dockerfile
    ğŸ“„ docker-compose.yml
    ğŸ“„ main.py
    ğŸ“„ nginx.conf
    ğŸ“„ requirements.txt
    ğŸ“‚ app/
        ğŸ“‚ core/
            ğŸ“„ __init__.py
            ğŸ“„ config.py
        ğŸ“‚ providers/
            ğŸ“„ __init__.py
            ğŸ“„ base_provider.py
            ğŸ“„ rawbot_provider.py
        ğŸ“‚ utils/
            ğŸ“„ sse_utils.py
================================================================================

--- æ–‡ä»¶è·¯å¾„: .env ---

# ====================================================================
# rawbot-2api é…ç½®æ–‡ä»¶æ¨¡æ¿ (v1.2 - OpenAI ç§»é™¤ç‰ˆ)
# ====================================================================
#
# è¯·å°†æ­¤æ–‡ä»¶é‡å‘½åä¸º ".env" å¹¶å¡«å…¥æ‚¨çš„å‡­è¯ã€‚
#

# --- æ ¸å¿ƒå®‰å…¨é…ç½® (å¿…é¡»è®¾ç½®) ---
# ç”¨äºä¿æŠ¤æ‚¨ API æœåŠ¡çš„è®¿é—®å¯†é’¥ã€‚
API_MASTER_KEY=1

# --- éƒ¨ç½²é…ç½® (å¯é€‰) ---
# Nginx å¯¹å¤–æš´éœ²çš„ç«¯å£
NGINX_PORT=8090

# --- ä¸‹æ¸¸æœåŠ¡å‡­è¯ (å¿…é¡»è®¾ç½®) ---
# ä»åŸå§‹ç½‘ç»œè¯·æ±‚ä¸­æå–çš„ Bearer Tokens
COHERE_TOKEN="5vwtfzo14cl7SDqDb1s5PB8gPhlYgNTRPiqR94q0"
AI21_TOKEN="Hlk4epA2ZO5MNj8p38FjnnkzZ6OGwZDH"
MISTRAL_TOKEN="4hAD3tANyfX6KGAo3WbjvzjVonABcZrb"


--- æ–‡ä»¶è·¯å¾„: .env.example ---

# ====================================================================
# rawbot-2api é…ç½®æ–‡ä»¶æ¨¡æ¿ (v1.2 - OpenAI ç§»é™¤ç‰ˆ)
# ====================================================================
#
# è¯·å°†æ­¤æ–‡ä»¶é‡å‘½åä¸º ".env" å¹¶å¡«å…¥æ‚¨çš„å‡­è¯ã€‚
#

# --- æ ¸å¿ƒå®‰å…¨é…ç½® (å¿…é¡»è®¾ç½®) ---
# ç”¨äºä¿æŠ¤æ‚¨ API æœåŠ¡çš„è®¿é—®å¯†é’¥ã€‚
API_MASTER_KEY=sk-rawbot-2api-default-key-please-change-me

# --- éƒ¨ç½²é…ç½® (å¯é€‰) ---
# Nginx å¯¹å¤–æš´éœ²çš„ç«¯å£
NGINX_PORT=8090

# --- ä¸‹æ¸¸æœåŠ¡å‡­è¯ (å¿…é¡»è®¾ç½®) ---
# ä»åŸå§‹ç½‘ç»œè¯·æ±‚ä¸­æå–çš„ Bearer Tokens
COHERE_TOKEN="5vwtfzo14cl7SDqDb1s5PB8gPhlYgNTRPiqR94q0"
AI21_TOKEN="Hlk4epA2ZO5MNj8p38FjnnkzZ6OGwZDH"
MISTRAL_TOKEN="4hAD3tANyfX6KGAo3WbjvzjVonABcZrb"


--- æ–‡ä»¶è·¯å¾„: Dockerfile ---

# ====================================================================
# Dockerfile for rawbot-2api (v1.0 - Concurrent Fan-Out Edition)
# ====================================================================

FROM python:3.10-slim

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
WORKDIR /app

# å®‰è£… Python ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºå¹¶åˆ‡æ¢åˆ°é root ç”¨æˆ·
RUN useradd --create-home appuser && \
    chown -R appuser:appuser /app
USER appuser

# æš´éœ²ç«¯å£å¹¶å¯åŠ¨
EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]


--- æ–‡ä»¶è·¯å¾„: docker-compose.yml ---

services:
  nginx:
    image: nginx:latest
    container_name: rawbot-2api-nginx
    restart: always
    ports:
      - "${NGINX_PORT:-8090}:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - app
    networks:
      - rawbot-net

  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rawbot-2api-app
    restart: unless-stopped
    env_file:
      - .env
    networks:
      - rawbot-net

networks:
  rawbot-net:
    driver: bridge


--- æ–‡ä»¶è·¯å¾„: main.py ---

import json
from contextlib import asynccontextmanager
from typing import Optional, Union

from fastapi import FastAPI, Request, HTTPException, Depends, Header
from fastapi.responses import JSONResponse, StreamingResponse
from loguru import logger

from app.core.config import settings
from app.providers.rawbot_provider import RawbotProvider

# æ—¥å¿—é…ç½®ä¿æŒä¸å˜
logger.add(
    "logs/app.log", 
    rotation="10 MB", 
    retention="7 days", 
    level="INFO",
    format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}",
    encoding="utf-8"
)
logger.add(
    lambda msg: print(msg, end=''),
    level="INFO",
    format="<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <cyan>{name}:{function}:{line}</cyan> - <level>{message}</level>",
    colorize=True
)

provider = RawbotProvider()

@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info(f"åº”ç”¨å¯åŠ¨ä¸­... {settings.APP_NAME} v{settings.APP_VERSION}")
    logger.info("æœåŠ¡å·²è¿›å…¥ 'Dual-Mode Smart Response' æ¨¡å¼ã€‚")
    logger.info(f"æœåŠ¡å°†åœ¨ http://localhost:{settings.NGINX_PORT} ä¸Šå¯ç”¨")
    yield
    logger.info("åº”ç”¨å…³é—­ã€‚")

app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    description=settings.DESCRIPTION,
    lifespan=lifespan
)

async def verify_api_key(authorization: Optional[str] = Header(None)):
    if settings.API_MASTER_KEY and settings.API_MASTER_KEY != "1":
        if not authorization or "bearer" not in authorization.lower():
            raise HTTPException(status_code=401, detail="éœ€è¦ Bearer Token è®¤è¯ã€‚")
        token = authorization.split(" ")[-1]
        if token != settings.API_MASTER_KEY:
            raise HTTPException(status_code=403, detail="æ— æ•ˆçš„ API Keyã€‚")

# --- [æ ¸å¿ƒä¿®æ­£] ---
# æ·»åŠ  response_model=None æ¥è§£å†³ FastAPI å¯åŠ¨é”™è¯¯
@app.post(
    "/v1/chat/completions",
    dependencies=[Depends(verify_api_key)],
    response_model=None 
)
async def chat_completions(request: Request) -> Union[JSONResponse, StreamingResponse]:
    try:
        request_data = await request.json()
        
        response = await provider.chat_completion(request_data)
        
        if isinstance(response, JSONResponse):
             # ç¡®ä¿ response.body æ˜¯å­—èŠ‚ä¸²
             body_bytes = response.body
             logger.info(f"--- [å‘é€éæµå¼å“åº”] ---\n{json.dumps(json.loads(body_bytes.decode('utf-8')), indent=2, ensure_ascii=False)}")

        return response
    except Exception as e:
        logger.error(f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {str(e)}")

@app.get("/v1/models", dependencies=[Depends(verify_api_key)], response_class=JSONResponse)
async def list_models():
    # provider.get_models() å·²ç»è¿”å› JSONResponseï¼Œç›´æ¥è¿”å›å³å¯
    return await provider.get_models()

@app.get("/", summary="æ ¹è·¯å¾„", include_in_schema=False)
def root():
    return {"message": f"æ¬¢è¿æ¥åˆ° {settings.APP_NAME} v{settings.APP_VERSION}. æœåŠ¡è¿è¡Œæ­£å¸¸ã€‚"}


--- æ–‡ä»¶è·¯å¾„: nginx.conf ---

worker_processes auto;

events {
    worker_connections 1024;
}

http {
    upstream rawbot_backend {
        # ip_hash is not strictly necessary here as the app is stateless,
        # but it's good practice for potential future stateful features.
        ip_hash;
        server app:8000;
    }

    server {
        listen 80;
        server_name localhost;

        location / {
            proxy_pass http://rawbot_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Standard proxy settings
            proxy_buffering off;
            proxy_cache off;
            proxy_set_header Connection '';
            proxy_http_version 1.1;
            chunked_transfer_encoding off;
        }
    }
}


--- æ–‡ä»¶è·¯å¾„: requirements.txt ---

fastapi
uvicorn[standard]
pydantic-settings
python-dotenv
httpx
loguru


--- æ–‡ä»¶è·¯å¾„: app\core\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: app\core\config.py ---

from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import List, Optional

class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding='utf-8',
        extra="ignore"
    )

    APP_NAME: str = "rawbot-2api"
    APP_VERSION: str = "1.2.0"
    DESCRIPTION: str = "ä¸€ä¸ªå°† rawbot.org çš„å¤šæ¨¡å‹æ¯”è¾ƒåŠŸèƒ½èšåˆä¸ºå•ä¸€ API çš„é«˜æ€§èƒ½ä»£ç† (å·²ç§»é™¤ OpenAI)ã€‚"

    # --- å®‰å…¨ä¸éƒ¨ç½² ---
    API_MASTER_KEY: Optional[str] = None
    NGINX_PORT: int = 8090

    # --- ä¸‹æ¸¸å‡­è¯ ---
    COHERE_TOKEN: Optional[str] = None
    AI21_TOKEN: Optional[str] = None
    MISTRAL_TOKEN: Optional[str] = None

    # --- API è¡Œä¸º ---
    API_REQUEST_TIMEOUT: int = 120

    # --- æ¨¡å‹å®šä¹‰ ---
    VIRTUAL_MODEL: str = "rawbot-omnibus"
    KNOWN_MODELS: List[str] = [
        "rawbot-omnibus",
        "command-r-08-2024",
        "jamba-mini",
        "mistral-small-latest"
    ]

settings = Settings()


--- æ–‡ä»¶è·¯å¾„: app\providers\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: app\providers\base_provider.py ---

from abc import ABC, abstractmethod
from typing import Dict, Any
from fastapi.responses import JSONResponse

class BaseProvider(ABC):
    @abstractmethod
    async def chat_completion(self, request_data: Dict[str, Any]) -> JSONResponse:
        pass

    @abstractmethod
    async def get_models(self) -> JSONResponse:
        pass


--- æ–‡ä»¶è·¯å¾„: app\providers\rawbot_provider.py ---

import httpx
import asyncio
import time
import uuid
from typing import Dict, Any, List, AsyncGenerator
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi import HTTPException
from loguru import logger

from app.core.config import settings
from app.providers.base_provider import BaseProvider
from app.utils.sse_utils import create_chat_completion_chunk, create_sse_data, DONE_CHUNK

class RawbotProvider(BaseProvider):
    def __init__(self):
        self.client = httpx.AsyncClient(timeout=settings.API_REQUEST_TIMEOUT)
        self.providers = [
            {"name": "Cohere", "model": "command-r-08-2024", "token": settings.COHERE_TOKEN, "url": "https://api.cohere.ai/v1/chat", "method": self._call_cohere},
            {"name": "AI21 Labs", "model": "jamba-mini", "token": settings.AI21_TOKEN, "url": "https://api.ai21.com/studio/v1/chat/completions", "method": self._call_ai21},
            {"name": "Mistral", "model": "mistral-small-latest", "token": settings.MISTRAL_TOKEN, "url": "https://api.mistral.ai/v1/chat/completions", "method": self._call_mistral},
        ]

    async def chat_completion(self, request_data: Dict[str, Any]):
        user_prompt = next((m['content'] for m in reversed(request_data.get("messages", [])) if m.get('role') == 'user'), None)
        if not user_prompt:
            raise HTTPException(status_code=400, detail="åœ¨ 'messages' ä¸­æœªæ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯ã€‚")

        # æ ¹æ®å®¢æˆ·ç«¯è¯·æ±‚å†³å®šå“åº”æ¨¡å¼
        if request_data.get("stream", False):
            logger.info("æ£€æµ‹åˆ°æµå¼è¯·æ±‚ (stream: true)ï¼Œå¯åŠ¨ä¼ªæµå¼ç”Ÿæˆå™¨ã€‚")
            return StreamingResponse(self._stream_response_generator(user_prompt), media_type="text/event-stream")
        else:
            logger.info("å¤„ç†éæµå¼è¯·æ±‚ (stream: false)ã€‚")
            content = await self._get_aggregated_content(user_prompt)
            response_data = self._build_non_stream_response(content)
            return JSONResponse(content=response_data)

    async def _get_aggregated_content(self, prompt: str) -> str:
        """åœ¨å†…éƒ¨å¹¶å‘è·å–æ‰€æœ‰ç»“æœå¹¶æ ¼å¼åŒ–ä¸ºçº¯æ–‡æœ¬ã€‚"""
        tasks = [provider["method"](prompt, provider) for provider in self.providers]
        results = await asyncio.gather(*tasks)
        return self._format_plain_text_response(results)

    async def _stream_response_generator(self, prompt: str) -> AsyncGenerator[str, None]:
        """ä¼ªæµå¼ç”Ÿæˆå™¨ï¼Œé€å­—å‘é€å†…å®¹ã€‚"""
        request_id = f"chatcmpl-{uuid.uuid4()}"
        model_name = settings.VIRTUAL_MODEL
        
        try:
            full_content = await self._get_aggregated_content(prompt)
            
            # é€å­—å‘é€ï¼Œæ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœ
            for char in full_content:
                chunk = create_chat_completion_chunk(request_id, model_name, char)
                sse_event = create_sse_data(chunk)
                logger.info(f"--- [æµå¼å‘é€] ---\n{sse_event.strip()}")
                yield sse_event
                await asyncio.sleep(0.01) # æ§åˆ¶æ‰“å­—é€Ÿåº¦

            # å‘é€ç»“æŸæ ‡å¿—
            final_chunk = create_chat_completion_chunk(request_id, model_name, "", "stop")
            final_event = create_sse_data(final_chunk)
            logger.info(f"--- [æµå¼ç»“æŸ] ---\n{final_event.strip()}")
            yield final_event
            yield DONE_CHUNK
            
        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆå™¨å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            error_chunk = create_chat_completion_chunk(request_id, model_name, f"\næœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {e}", "stop")
            yield create_sse_data(error_chunk)
            yield DONE_CHUNK

    def _build_non_stream_response(self, content: str) -> Dict[str, Any]:
        """æ„å»ºå®Œæ•´çš„éæµå¼JSONå“åº”ã€‚"""
        return {
            "id": f"chatcmpl-{uuid.uuid4()}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": settings.VIRTUAL_MODEL,
            "choices": [{"index": 0, "message": {"role": "assistant", "content": content}, "finish_reason": "stop"}],
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        }

    def _format_plain_text_response(self, results: List[Dict]) -> str:
        response_parts = []
        for result in results:
            provider_name = result.get("provider", "æœªçŸ¥")
            model_name = result.get("model", "N/A")
            header = f"--- [{provider_name} ({model_name})] ---\n"
            
            if result.get("error"):
                content = f"è¯·æ±‚å¤±è´¥: {result['content']}\n"
            else:
                content = result.get("content", "æ— å“åº”å†…å®¹ã€‚\n")
            
            response_parts.append(header + content)
        
        return "\n".join(response_parts)

    # _call_* æ–¹æ³•ä¿æŒä¸å˜
    async def _call_cohere(self, prompt: str, provider_info: Dict) -> Dict:
        payload = {"model": provider_info["model"], "message": prompt, "temperature": 0.5, "max_tokens": 200}
        headers = {"Authorization": f"Bearer {provider_info['token']}", "Content-Type": "application/json"}
        try:
            response = await self.client.post(provider_info["url"], headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()
            return {"provider": provider_info["name"], "model": provider_info["model"], "content": data.get("text", "")}
        except Exception as e:
            logger.error(f"è¯·æ±‚ {provider_info['name']} å¤±è´¥: {e}")
            return {"provider": provider_info["name"], "model": provider_info["model"], "content": str(e), "error": True}

    async def _call_ai21(self, prompt: str, provider_info: Dict) -> Dict:
        payload = {"model": provider_info["model"], "messages": [{"role": "user", "content": prompt}], "temperature": 0.5, "max_tokens": 200}
        headers = {"Authorization": f"Bearer {provider_info['token']}", "Content-Type": "application/json"}
        try:
            response = await self.client.post(provider_info["url"], headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()
            return {"provider": provider_info["name"], "model": provider_info["model"], "content": data["choices"][0]["message"]["content"]}
        except Exception as e:
            logger.error(f"è¯·æ±‚ {provider_info['name']} å¤±è´¥: {e}")
            return {"provider": provider_info["name"], "model": provider_info["model"], "content": str(e), "error": True}

    async def _call_mistral(self, prompt: str, provider_info: Dict) -> Dict:
        payload = {"model": provider_info["model"], "messages": [{"role": "user", "content": prompt}], "temperature": 0.5, "max_tokens": 200}
        headers = {"Authorization": f"Bearer {provider_info['token']}", "Content-Type": "application/json"}
        try:
            response = await self.client.post(provider_info["url"], headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()
            return {"provider": provider_info["name"], "model": provider_info["model"], "content": data["choices"][0]["message"]["content"]}
        except Exception as e:
            logger.error(f"è¯·æ±‚ {provider_info['name']} å¤±è´¥: {e}")
            return {"provider": provider_info["name"], "model": provider_info["model"], "content": str(e), "error": True}

    async def get_models(self) -> JSONResponse:
        model_data = {
            "object": "list",
            "data": [{"id": name, "object": "model", "created": int(time.time()), "owned_by": "lzA6"} for name in settings.KNOWN_MODELS]
        }
        return JSONResponse(content=model_data)


--- æ–‡ä»¶è·¯å¾„: app\utils\sse_utils.py ---

import json
import time
from typing import Dict, Any, Optional

DONE_CHUNK = "data: [DONE]\n\n"

def create_sse_data(data: Dict[str, Any]) -> str:
    """å°†å­—å…¸æ•°æ®æ ¼å¼åŒ–ä¸º SSE äº‹ä»¶å­—ç¬¦ä¸²ã€‚"""
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

def create_chat_completion_chunk(
    request_id: str,
    model: str,
    content: str,
    finish_reason: Optional[str] = None
) -> Dict[str, Any]:
    """åˆ›å»ºä¸€ä¸ªä¸ OpenAI å…¼å®¹çš„èŠå¤©è¡¥å…¨æµå¼å—ã€‚"""
    return {
        "id": request_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "delta": {"content": content},
                "finish_reason": finish_reason
            }
        ]
    }



